{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Neural style transfer\n",
    "In addition to DeepDream, another major development in deep-learning-driven image modification is neural style transfer. The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction in the summer of 2015, and it has made its way into many smartphone photo apps.\n",
    "\n",
    "Neural style transfer consists of applying the style of a reference image to a target image while conserving the content of the target image. Here is an example:\n",
    "\n",
    "![style_transfer](images/8_3_0_styletransfer.png)\n",
    "\n",
    "*Style* essentially means textures, colors, and visual patterns in the image, at various spatial scales, and the content is a higher-level macrostructure of the image. For example, blue-and-yellow circular brushstrokes are considered to be the style used by Vincent Van Gogh in the painting above, and the buildings in the Tubingen photograph are considered to be the content.\n",
    "\n",
    "As it turns out, the deep-learning-based implementations of style transfer offer results unparalleled by what had been previously achieved with classical computer-vision techniques, and they triggered an amazing renaissance in creative applications of computer vision.\n",
    "\n",
    "The key notion behind implementing style transfer is the same idea that’s central to all deep-learning algorithms: define a loss function to specify what we want to achieve, and then we minimize this loss. We know what we want to achieve: conserving the content of the original image while adopting the style of the reference image. If we were able to mathematically define content and style, then an appropriate loss function to minimize would be the following:\n",
    "\n",
    "`loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))`\n",
    "       \n",
    "Here, `distance` is a norm function such that the L2 norm, `content` is a function that takes an image and computes a representation of its content, and `style` is a function that takes an image and computes a representation of its style. Minimizing this loss causes `style(generated_image)` to be close to `style(reference_image)`, and `content(generated_image)` is close to `content(generated_image)`, thus achieving style transfer as we defined it.\n",
    "\n",
    "A fundamental observation made was that deep convolutional neural networks offer a way to mathematically define the `style` and `content` functions. Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.1 The content loss\n",
    "Activations from earlier layers in a network contain local information about the image, whereas activations from higher layers contain increasingly global, abstract information. We would expect the content of an image, which is more global and abstract, to be captured by the representations of the upper layers in a convnet.\n",
    "\n",
    "A good candidate for content loss is the L2 norm between the activations of an upper layer in a pretrained convnet, computed over the target image, and the activations of the same layer computed over the generated image. This guarantees that the generated image will look similar to the original target image.\n",
    "\n",
    "## 8.3.2 The style loss\n",
    "Whereas the content loss only uses a single upper layer, the style loss uses multiple layers of a convnet. The style loss aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. This guarantees that the textures found at different spatial scales look similar across the style-reference image and the generated image. We can use a pretrained convnet to define a loss that will do the following:\n",
    "\n",
    " - Preserve content by maintaining similar high-level layer activations between the target content image and the generated image. The convnet should “see” both the target image and the generated image as containing the same things.\n",
    " - Preserve style by maintaining similar correlations within activations for both low-level layers and high-level layers.\n",
    "\n",
    "Now, let’s look at a Keras implementation of the original 2015 neural style transfer algorithm. It shares many similarities with the DeepDream implementation.\n",
    "\n",
    "## 8.3.3 Neural style transfer in Keras\n",
    "Neural style transfer can be implemented using any pretrained convnet. We’ll use the VGG19 network which is a simple variant of the VGG16 network, with three more convolutional layers. This is the general process:\n",
    "\n",
    " 1. Set up a network that computes VGG19 layer activations for the style-reference image, the target image, and the generated image at the same time.\n",
    " 2. Use the layer activations computed over these three images to define the loss function described earlier, which we’ll minimize in order to achieve style transfer.\n",
    " 3. Set up a gradient-descent process to minimize this loss function.\n",
    " \n",
    "We will start by defining the paths to the style-reference image and the target image. To make sure that the processed images are a similar size (different sizes make style transfer more difficult), we will resize them all to a shared height of 400 px.\n",
    "\n",
    "For the target image, I used a picture of Abraham Lincoln (below).\n",
    "\n",
    "![abe](images/abe.jpg)\n",
    "\n",
    "And the style-reference image is of characters from the video game Fortnite (below). Our goal is to generate a new image that looks like Abe Lincoln, but in the style of Fortnite. Let's get to coding and see what we are able to create!\n",
    "\n",
    "![fortnite](images/fortnite.jpg)\n",
    "\n",
    "**DEFINING INITIAL VARIABLES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "target_image_path = 'images/abe.jpg' # path to image we are going to transform\n",
    "style_reference_image_path = 'images/fortnite.jpg' # path to image whose style we want to mimic\n",
    "\n",
    "# Set dimensions of the generated picture\n",
    "width, height = load_img(target_image_path).size\n",
    "img_height = 400\n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need some helper functions for loading, preprocessing, and postprocessing the images that go in and out of the VGG19 convnet.\n",
    "\n",
    "**HELPER FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def postprocess_image(x):\n",
    "    # zero-centering by removing mean pxl value from ImageNet. This reverses an unwanted vgg19 transformation.\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    # Convert image from BGR to RGB\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up the VGG19 network. It takes a batch of three images as input: the style-reference image, the target image, and a placeholder that will contain the generated image. The placeholder is a symbolic tensor, with values that are provided externally via Numpy arrays. The style-reference and target images are static whereas the values contained in the placeholder of the generated image will change over time.\n",
    "\n",
    "**LOADING THE PRETRAINED VGG19 NETWORK & APPLYING IT TO 3 IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining input images into single batch.\n",
      "Loading model.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80134144/80134624 [============================>.] - ETA: 0sModel loaded.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
    "combination_image = K.placeholder((1, img_height, img_width, 3)) # Placeholder that will contain generated image\n",
    "\n",
    "print 'Combining input images into single batch.'\n",
    "# Combine the 3 images into a single batch\n",
    "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n",
    "\n",
    "print 'Loading model.'\n",
    "# Build the VGG19 network with batch of 3 images. Model is loaded with pretrained ImageNet weights\n",
    "model = vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "\n",
    "print 'Model loaded.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define the content loss, which will ensure the top layer of the VGG19 convnet has a similar view of the target and generated images.\n",
    "\n",
    "**CONTENT LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will define the style loss. It uses an auxiliary function to compute the Gram matrix of an input matrix. In our case, the input matrix is a map of the correlations found in the original feature matrix.\n",
    "\n",
    "**STYLE LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = img_height * img_width\n",
    "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to content loss and style loss, we will add the total variation loss, which operates on the pixels of the generated combination image. This helps prevent the output image from being overly pixelated. We can interpret it as a regularization loss.\n",
    "\n",
    "**TOTAL VARIATION LOSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_variation_loss(x):\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, 1:, :img_width - 1, :])\n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] -\n",
    "        x[:, :img_height - 1, 1:, :])\n",
    "    return K.sum(K.pow(a + b, 1.25))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss that we minimize is a weighted average of the content, style, and total variation losses. To compute the content loss, only use one upper layer - the block5_conv2 layer. For the style loss, we use a list of layers that spans both low-level and high-level layers. We then add the total variation loss at the end.\n",
    "\n",
    "Depending on the style-reference image and content image we use, we’ll probably want to adjust the `content_weight` coefficient. A higher `content_weight` means the target content will be more recognizable in the generated image.\n",
    "\n",
    "**DEFINING FINAL LOSS THAT WE WILL MINIMIZE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary that maps layer names to activation tensors\n",
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# Content loss layer\n",
    "content_layer = 'block5_conv2'\n",
    "\n",
    "# Style loss layers\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "# Weights in the weighted average of the loss components\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025 # A higher content_weight means the generated image will look more like our target img (Abe)\n",
    "\n",
    "# Add the content loss\n",
    "loss = K.variable(0.) # Define loss by adding all components to this scalar variable\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "loss += content_weight * content_loss(target_image_features, combination_features)\n",
    "\n",
    "# Add a style loss component for each target layer\n",
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    sl = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * sl\n",
    "    \n",
    "# Add the total variation loss\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll set up the gradient-descent process. In the original Gatys paper that introduced neural style transfer, optimization is performed using the L-BFGS algorithm, so that’s what we’ll use here. This is a key difference from the DeepDream example. The L-BFGS algorithm comes packaged with SciPy, but there are two slight limitations with the SciPy implementation:\n",
    "\n",
    " - It requires that you pass the value of the loss function and the value of the gradients as two separate functions.\n",
    " - It can only be applied to flat vectors, whereas we have a 3D image array.\n",
    "\n",
    "It would be inefficient to compute the value of the loss function and the value of the gradients independently, because doing so would lead to a lot of redundant computation between the two; the process would be almost twice as slow as computing them at the same time. To bypass this, we’ll set up a Python class called `Evaluator` that computes both the loss value and the gradients value at once, then returns the loss value when called the first time, and stores the gradients to be used on the next call.\n",
    "\n",
    "**SETTING UP GRADIENT-DESCENT PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the gradients of the generated image with regard to the loss\n",
    "grads = K.gradients(loss, combination_image)[0]\n",
    "\n",
    "# Function to fetch values of current loss & current gradients\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
    "\n",
    "# Evaluator class wraps `fetch_loss_and_grads` & lets us retrieve losses & gradients w/ 2 separate method calls...\n",
    "# which is required by SciPy optimizer we will use\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype('float64')\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run the gradient-ascent process using SciPy’s L-BFGS algorithm. We will save the current generated image at each iteration of the algorithm (here, a single iteration represents 20 steps of gradient ascent).\n",
    "\n",
    "**STYLE-TRANSFER LOOP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start of iteration', 0)\n",
      "('Current loss value:', 4539376600.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_0.png')\n",
      "Iteration 0 completed in 639s\n",
      "('Start of iteration', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Current loss value:', 1744123100.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_1.png')\n",
      "Iteration 1 completed in 587s\n",
      "('Start of iteration', 2)\n",
      "('Current loss value:', 1086659500.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_2.png')\n",
      "Iteration 2 completed in 604s\n",
      "('Start of iteration', 3)\n",
      "('Current loss value:', 813086600.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_3.png')\n",
      "Iteration 3 completed in 599s\n",
      "('Start of iteration', 4)\n",
      "('Current loss value:', 670284540.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_4.png')\n",
      "Iteration 4 completed in 618s\n",
      "('Start of iteration', 5)\n",
      "('Current loss value:', 577944450.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_5.png')\n",
      "Iteration 5 completed in 768s\n",
      "('Start of iteration', 6)\n",
      "('Current loss value:', 510998180.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_6.png')\n",
      "Iteration 6 completed in 844s\n",
      "('Start of iteration', 7)\n",
      "('Current loss value:', 459679650.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_7.png')\n",
      "Iteration 7 completed in 599s\n",
      "('Start of iteration', 8)\n",
      "('Current loss value:', 426596200.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_8.png')\n",
      "Iteration 8 completed in 595s\n",
      "('Start of iteration', 9)\n",
      "('Current loss value:', 395992500.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_9.png')\n",
      "Iteration 9 completed in 756s\n",
      "('Start of iteration', 10)\n",
      "('Current loss value:', 370279460.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_10.png')\n",
      "Iteration 10 completed in 813s\n",
      "('Start of iteration', 11)\n",
      "('Current loss value:', 345108770.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_11.png')\n",
      "Iteration 11 completed in 651s\n",
      "('Start of iteration', 12)\n",
      "('Current loss value:', 327271700.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_12.png')\n",
      "Iteration 12 completed in 544s\n",
      "('Start of iteration', 13)\n",
      "('Current loss value:', 310433570.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_13.png')\n",
      "Iteration 13 completed in 556s\n",
      "('Start of iteration', 14)\n",
      "('Current loss value:', 297477860.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_14.png')\n",
      "Iteration 14 completed in 609s\n",
      "('Start of iteration', 15)\n",
      "('Current loss value:', 285069500.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_15.png')\n",
      "Iteration 15 completed in 592s\n",
      "('Start of iteration', 16)\n",
      "('Current loss value:', 273854620.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_16.png')\n",
      "Iteration 16 completed in 608s\n",
      "('Start of iteration', 17)\n",
      "('Current loss value:', 264561330.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_17.png')\n",
      "Iteration 17 completed in 714s\n",
      "('Start of iteration', 18)\n",
      "('Current loss value:', 255559540.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_18.png')\n",
      "Iteration 18 completed in 737s\n",
      "('Start of iteration', 19)\n",
      "('Current loss value:', 246902450.0)\n",
      "('Image saved as', 'abe_fortnite_at_iteration_19.png')\n",
      "Iteration 19 completed in 868s\n"
     ]
    }
   ],
   "source": [
    "result_prefix = 'abe_fortnite'\n",
    "iterations = 20\n",
    "\n",
    "# Initial state: the target image\n",
    "x = preprocess_image(target_image_path)\n",
    "\n",
    "# Flatten image so it can be processed by SciPy\n",
    "x = x.flatten()\n",
    "\n",
    "# Run L-BFGS over pixels of generated img to minimize neural style loss\n",
    "# Note: have to pass function that computes loss & function that computes gradients separately\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    \n",
    "    # Save the current generated image\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = postprocess_image(img)\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    imsave(fname, img)\n",
    "    print('Image saved as', fname)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's see some of the results! After 20 iterations, here is our Fortnite Abe creation!\n",
    "\n",
    "![fortniteabe](abe_fortnite_at_iteration_19.png)\n",
    "\n",
    "Some additional examples are shown below. It's important to keep in mind that this technique merely achieves a form of image retexturing, or texture transfer. It works best with style-reference images that are strongly textured and highly self-similar, and with content targets that don’t require high levels of detail in order to be recognizable. It typically can’t achieve fairly abstract feats such as transferring the style of one portrait to another. The algorithm is closer to classical signal processing than to AI, so don’t expect it to work like magic!\n",
    "\n",
    "![neural_transfer](images/8_3_3_neural_transfer.png)\n",
    "\n",
    "Additionally, note that running this style-transfer algorithm is slow. But the transformation operated by the setup is simple enough that it can be learned by a small, fast feedforward convnet. Fast style transfer can be achieved by initially spending several compute cycles to generate input-output training examples for a fixed style-reference image, using the method outlined here, and then training a simple convnet to learn this style-specific transformation. Once that’s done, stylizing a given image is instantaneous: it’s just a forward pass of this small convnet.\n",
    "\n",
    "## 8.3.4. Wrapping up\n",
    " - Style transfer consists of creating a new image that preserves the contents of a target image while also capturing the style of a reference image.\n",
    " - Content can be captured by the high-level activations of a convnet.\n",
    " - Style can be captured by the internal correlations of the activations of different layers of a convnet.\n",
    " - Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet.\n",
    " - Starting from this basic idea, many variants and refinements are possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
