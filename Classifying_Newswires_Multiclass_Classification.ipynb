{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying newswires: a multiclass classification example\n",
    "In this example, we will build a network to classify Reuters newswires into 46 mutually exclusive topics. Because there are many classes and each data point should be classified into only one category, this problem is considered to be a *single-label multiclass classification*. \n",
    "\n",
    "### 3.5.1 The Reuters dataset\n",
    "The Reuters dataset is a set of short newswires and their topics published in 1986. Let's load it from Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 14s    \n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "# num_words limits to 10,000 most frequently occurring words\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  8982\n",
      "Number of test examples:  2246\n"
     ]
    }
   ],
   "source": [
    "print \"Number of training examples: \", len(train_data)\n",
    "print \"Number of test examples: \", len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 245,\n",
       " 273,\n",
       " 207,\n",
       " 156,\n",
       " 53,\n",
       " 74,\n",
       " 160,\n",
       " 26,\n",
       " 14,\n",
       " 46,\n",
       " 296,\n",
       " 26,\n",
       " 39,\n",
       " 74,\n",
       " 2979,\n",
       " 3554,\n",
       " 14,\n",
       " 46,\n",
       " 4689,\n",
       " 4329,\n",
       " 86,\n",
       " 61,\n",
       " 3499,\n",
       " 4795,\n",
       " 14,\n",
       " 61,\n",
       " 451,\n",
       " 4329,\n",
       " 17,\n",
       " 12]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view example\n",
    "train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example is a list of intergers (word indices) corresponding to specific words. Here's how we can decode it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
      "540672/550378 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# decode review back to English\n",
    "word_index = reuters.get_word_index() #dictionary mapping words to integer index\n",
    "\n",
    "# this reverses the dictionary so we return words instead of numbers\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# This decodes the review. Note indices are offset by 3 because 0, 1, & 2 are...\n",
    "# reserved indices for \"padding,\" \"start of sequence,\" and \"unknown\"\n",
    "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_newswire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label associated with an example is number between 0 and 45\n",
    "train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Preparing the data\n",
    "Let's vectorize the data.\n",
    "\n",
    "You can't feed lists of integers into a neural network. You must turn your lists into tensors. There are two ways to do that:\n",
    " - Pad lists so they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use the first layer in the network as a layer capable of handling such integer tensors (the Embedding layer).\n",
    " - **OR** One-hot encode the lists to turn them into vectors of 0s and 1s. This would mean turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then you could use the first layer as a Dense Layer, capable of handling floating-point vector data.\n",
    "\n",
    "We will go with the 2nd option of vectorizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension)) # creates an all-zero matrix\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1. # Sets specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# vectorize training data\n",
    "x_train = vectorize_sequences(train_data)\n",
    "\n",
    "# vectorize testing data\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To vectorize the labels, there are also two possibilities:\n",
    " - Cast the label list as an integer tensor\n",
    " - **OR** use one-hot encoding (also called categorical encoding).\n",
    " \n",
    "Here we will use one-hot encoding which consists of embedding each label as an all-zero vector with a 1 in the place of the label index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize labels\n",
    "\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "\n",
    "# vectorize training labels\n",
    "one_hot_train_labels = to_one_hot(train_labels)\n",
    "\n",
    "# vectorize testing labels\n",
    "one_hot_test_labels = to_one_hot(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can also use a built-in Keras utility\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print train_labels[10]\n",
    "print one_hot_train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the exact change we just made.\n",
    "\n",
    "### 3.5.3 Building the network\n",
    "Unlike classifying movie reviews where there were two classes: positive review or negative review, the number of output classes for classifying newswires has gone from 2 to 46, making the dimensionality of the output space much larger.\n",
    "\n",
    "In a stack of `Dense` layers, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers, each layer potentially becoming a layer bottleneck. In the movie classification we used 16-dimensional intermediate layers, but 16 may be too little to learn to separate 46 different classes, so we will instead use 64 units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# use softmax activation instead of sigmoid\n",
    "model.add(layers.Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things to note about our network:\n",
    " - The network ends with a `Dense` layer of size 46. For each input sample, the network will output a 46-dimensional vector, and each entry in this vector will encode a different output class.\n",
    " - The last layer uses a `softmax` activation. This means the network will output a *probability distribution* over the 46 different output classes.\n",
    " \n",
    "The best loss function to use in this case is `categorical_crossentropy` because it will measure the distance between the probability distribution output by the network and the true distribution of the labels. By minimizing the distance between these two distributions, the network is trained to output something as close as possible to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.4 Validating approach\n",
    "Let's set apart 1,000 samples from the training data to use as the validation set, and then train the network for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s - loss: 2.5306 - acc: 0.4962 - val_loss: 1.7180 - val_acc: 0.6120\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 1s - loss: 1.4430 - acc: 0.6878 - val_loss: 1.3435 - val_acc: 0.7060\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 1s - loss: 1.0929 - acc: 0.7661 - val_loss: 1.1704 - val_acc: 0.7430\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.8682 - acc: 0.8166 - val_loss: 1.0788 - val_acc: 0.7600\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.7020 - acc: 0.8483 - val_loss: 0.9844 - val_acc: 0.7830\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.5666 - acc: 0.8796 - val_loss: 0.9401 - val_acc: 0.8030\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4592 - acc: 0.9039 - val_loss: 0.9090 - val_acc: 0.8010\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.3704 - acc: 0.9226 - val_loss: 0.9359 - val_acc: 0.7890\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.3036 - acc: 0.9308 - val_loss: 0.8912 - val_acc: 0.8070\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.2539 - acc: 0.9412 - val_loss: 0.9059 - val_acc: 0.8110\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.2185 - acc: 0.9471 - val_loss: 0.9152 - val_acc: 0.8120\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1872 - acc: 0.9511 - val_loss: 0.9045 - val_acc: 0.8150\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1696 - acc: 0.9523 - val_loss: 0.9338 - val_acc: 0.8090\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1531 - acc: 0.9554 - val_loss: 0.9644 - val_acc: 0.80900.955\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1387 - acc: 0.9555 - val_loss: 0.9696 - val_acc: 0.8120\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1310 - acc: 0.9562 - val_loss: 1.0274 - val_acc: 0.8040\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1215 - acc: 0.9578 - val_loss: 1.0309 - val_acc: 0.7950\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1194 - acc: 0.9579 - val_loss: 1.0461 - val_acc: 0.8070\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1135 - acc: 0.9595 - val_loss: 1.1006 - val_acc: 0.7950\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 1s - loss: 0.1104 - acc: 0.9598 - val_loss: 1.0705 - val_acc: 0.8020\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "                    batch_size=512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a59e313d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce48d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation accuracy\n",
    "plt.clf()\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network appears to overfit after 9 epochs, so we will train a new network from scratch for nine epochs then evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/9\n",
      "7982/7982 [==============================] - 2s - loss: 2.5401 - acc: 0.5226 - val_loss: 1.6792 - val_acc: 0.6540\n",
      "Epoch 2/9\n",
      "7982/7982 [==============================] - 2s - loss: 1.3785 - acc: 0.7096 - val_loss: 1.2825 - val_acc: 0.7210\n",
      "Epoch 3/9\n",
      "7982/7982 [==============================] - 2s - loss: 1.0207 - acc: 0.7781 - val_loss: 1.1321 - val_acc: 0.7550\n",
      "Epoch 4/9\n",
      "7982/7982 [==============================] - 2s - loss: 0.8003 - acc: 0.8257 - val_loss: 1.0533 - val_acc: 0.7580\n",
      "Epoch 5/9\n",
      "7982/7982 [==============================] - 2s - loss: 0.6392 - acc: 0.8629 - val_loss: 0.9746 - val_acc: 0.7950\n",
      "Epoch 6/9\n",
      "7982/7982 [==============================] - 2s - loss: 0.5111 - acc: 0.8931 - val_loss: 0.9097 - val_acc: 0.8130\n",
      "Epoch 7/9\n",
      "7982/7982 [==============================] - 1s - loss: 0.4115 - acc: 0.9144 - val_loss: 0.8916 - val_acc: 0.8230\n",
      "Epoch 8/9\n",
      "7982/7982 [==============================] - 1s - loss: 0.3356 - acc: 0.9287 - val_loss: 0.8732 - val_acc: 0.8270\n",
      "Epoch 9/9\n",
      "7982/7982 [==============================] - 1s - loss: 0.2786 - acc: 0.9370 - val_loss: 0.9353 - val_acc: 0.8020\n",
      "2240/2246 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# use softmax activation instead of sigmoid\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train, partial_y_train, epochs=9,\n",
    "          batch_size=512, validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0235643176658813, 0.77693677654461679]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach reaches an accuracy of about 78%. With a balanced binary classification problem, the accuracy reach by a purely random classifier would be around 50%, but in this case it's close to 18%, so the results seem comparatively good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.182546749777382"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "float(np.sum(hits_array)) / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.5 Generating predictions on new data\n",
    "Let's verify that the `predict` method of the model instance returns a probability distribution over all 46 topics. Each entry in `predictions` is a vector of length 46 where its coefficients sum to 1 and the largest entry is the predicted class- the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vector length =  (46,)\n",
      "Coefficients sum =  1.0\n",
      "Predicted class : 3\n",
      "Certainty Percentage:  0.97318\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "\n",
    "# vector length\n",
    "print \"Vector length = \", predictions[0].shape\n",
    "\n",
    "# vector sum\n",
    "print \"Coefficients sum = \", np.sum(predictions[0])\n",
    "\n",
    "# largest entry\n",
    "print \"Predicted class :\", np.argmax(predictions[0])\n",
    "print \"Certainty Percentage: \", np.max(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.6 A different way to handle the labels and the loss\n",
    "As mentioned earlier, another way to encode the labels would be to cast them as an integer tensor. The only thing this would change is the choice of the loss function. Instead of using `categorical_crossentropy`, we would use `sparse_categorical_crossentropy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print y_train[10]\n",
    "print y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.7 The importance of having sufficiently large intermediate layers\n",
    "Let's see what happens when we introduce an information bottleneck by having intermediate layers that are significantly less than 46-dimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 3s - loss: 2.6578 - acc: 0.3781 - val_loss: 1.9640 - val_acc: 0.5290\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.6599 - acc: 0.6196 - val_loss: 1.5352 - val_acc: 0.6230\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.3303 - acc: 0.6691 - val_loss: 1.3899 - val_acc: 0.6810\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.1407 - acc: 0.7285 - val_loss: 1.3165 - val_acc: 0.7000\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.0097 - acc: 0.7453 - val_loss: 1.2639 - val_acc: 0.7040\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.9079 - acc: 0.7581 - val_loss: 1.2720 - val_acc: 0.7100\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.8277 - acc: 0.7745 - val_loss: 1.2561 - val_acc: 0.7070\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.7662 - acc: 0.7897 - val_loss: 1.2555 - val_acc: 0.7180\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.7091 - acc: 0.8011 - val_loss: 1.2813 - val_acc: 0.7120\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.6588 - acc: 0.8117 - val_loss: 1.3221 - val_acc: 0.7080\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.6139 - acc: 0.8202 - val_loss: 1.3444 - val_acc: 0.7150\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.5747 - acc: 0.8336 - val_loss: 1.3806 - val_acc: 0.7160\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.5360 - acc: 0.8469 - val_loss: 1.4397 - val_acc: 0.7130\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.5025 - acc: 0.8613 - val_loss: 1.4610 - val_acc: 0.7180\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4738 - acc: 0.8678 - val_loss: 1.5096 - val_acc: 0.7160\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4450 - acc: 0.8790 - val_loss: 1.5429 - val_acc: 0.7170\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4230 - acc: 0.8815 - val_loss: 1.5784 - val_acc: 0.7180\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4029 - acc: 0.8896 - val_loss: 1.6215 - val_acc: 0.7220\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.3833 - acc: 0.8931 - val_loss: 1.6448 - val_acc: 0.7150\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.3668 - acc: 0.8958 - val_loss: 1.7340 - val_acc: 0.7080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5a10ff90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "          batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network now peaks at about 72% validation accuracy, down by over 5%. This is because we are trying to compress a lot of information into an intermediate space that is too low-dimensional.\n",
    "\n",
    "## 3.5.8 Additional Experiments\n",
    "### Using larger layers: 128 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 4s - loss: 1.5673 - acc: 0.6662 - val_loss: 1.0797 - val_acc: 0.7590\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.7696 - acc: 0.8282 - val_loss: 0.9420 - val_acc: 0.8090\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.4622 - acc: 0.8976 - val_loss: 0.8276 - val_acc: 0.8300\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.2981 - acc: 0.9341 - val_loss: 0.9672 - val_acc: 0.8020\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.2255 - acc: 0.9446 - val_loss: 0.9684 - val_acc: 0.8020\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1882 - acc: 0.9520 - val_loss: 0.9175 - val_acc: 0.8170\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1683 - acc: 0.9550 - val_loss: 0.9263 - val_acc: 0.8200\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1528 - acc: 0.9530 - val_loss: 0.9671 - val_acc: 0.8140\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1361 - acc: 0.9544 - val_loss: 1.2547 - val_acc: 0.7730\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1307 - acc: 0.9538 - val_loss: 1.0838 - val_acc: 0.8010\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1204 - acc: 0.9577 - val_loss: 1.1125 - val_acc: 0.8100\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1155 - acc: 0.9555 - val_loss: 1.0505 - val_acc: 0.8100\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1105 - acc: 0.9555 - val_loss: 1.1285 - val_acc: 0.8050\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1051 - acc: 0.9539 - val_loss: 1.1653 - val_acc: 0.8080\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.1005 - acc: 0.9587 - val_loss: 1.1993 - val_acc: 0.8070\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.0962 - acc: 0.9579 - val_loss: 1.2712 - val_acc: 0.7990\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.0934 - acc: 0.9562 - val_loss: 1.3359 - val_acc: 0.7960\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.0915 - acc: 0.9570 - val_loss: 1.3317 - val_acc: 0.7970\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.0879 - acc: 0.9570 - val_loss: 1.3997 - val_acc: 0.7930\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 4s - loss: 0.0836 - acc: 0.9567 - val_loss: 1.4361 - val_acc: 0.7970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a5905fd50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "          batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous Validation Accuracy: 81.5%. The max Validation Accuracy here was 83%**.\n",
    "\n",
    "### Using three hidden layers instead of two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7982 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.8940 - acc: 0.5882 - val_loss: 1.3137 - val_acc: 0.6790\n",
      "Epoch 2/20\n",
      "7982/7982 [==============================] - 2s - loss: 1.0165 - acc: 0.7646 - val_loss: 1.1070 - val_acc: 0.7430\n",
      "Epoch 3/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.6886 - acc: 0.8463 - val_loss: 0.9448 - val_acc: 0.7910\n",
      "Epoch 4/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.4744 - acc: 0.8978 - val_loss: 0.9564 - val_acc: 0.8000\n",
      "Epoch 5/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.3437 - acc: 0.9250 - val_loss: 0.9625 - val_acc: 0.8120\n",
      "Epoch 6/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.2694 - acc: 0.9386 - val_loss: 1.0245 - val_acc: 0.8130\n",
      "Epoch 7/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.2148 - acc: 0.9501 - val_loss: 1.0136 - val_acc: 0.8140\n",
      "Epoch 8/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1891 - acc: 0.9536 - val_loss: 1.1828 - val_acc: 0.7920\n",
      "Epoch 9/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1717 - acc: 0.9540 - val_loss: 1.1659 - val_acc: 0.8010\n",
      "Epoch 10/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1582 - acc: 0.9551 - val_loss: 1.1031 - val_acc: 0.8030\n",
      "Epoch 11/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1489 - acc: 0.9538 - val_loss: 1.1917 - val_acc: 0.7980\n",
      "Epoch 12/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1404 - acc: 0.9549 - val_loss: 1.3120 - val_acc: 0.7860\n",
      "Epoch 13/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1334 - acc: 0.9557 - val_loss: 1.1672 - val_acc: 0.8000\n",
      "Epoch 14/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1294 - acc: 0.9568 - val_loss: 1.2425 - val_acc: 0.7980\n",
      "Epoch 15/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1208 - acc: 0.9572 - val_loss: 1.2539 - val_acc: 0.7900\n",
      "Epoch 16/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1213 - acc: 0.9545 - val_loss: 1.2056 - val_acc: 0.8040\n",
      "Epoch 17/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1141 - acc: 0.9575 - val_loss: 1.3387 - val_acc: 0.7940\n",
      "Epoch 18/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1146 - acc: 0.9560 - val_loss: 1.2563 - val_acc: 0.7970\n",
      "Epoch 19/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1075 - acc: 0.9574 - val_loss: 1.3228 - val_acc: 0.7910\n",
      "Epoch 20/20\n",
      "7982/7982 [==============================] - 2s - loss: 0.1040 - acc: 0.9568 - val_loss: 1.3743 - val_acc: 0.7900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a59023990>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train, partial_y_train, epochs=20,\n",
    "          batch_size=128, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous Validation Accuracy: 81.5%. The max Validation Accuracy here was 81.4%**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
