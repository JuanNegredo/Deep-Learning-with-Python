{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Using a pretrained convnet\n",
    "A common and effective approach to deep learning on small image datasets is to use a pretrained network, which is a saved network that was previously trained on a large dataset. If the pretrained model was trained on a large enough dataset, then its features can prove useful for many different computer vision problems, even if they are unlike the new task you are training it to perform. Portability of learned features across different problems is a key advantage to deep learning compared to older, shallow-learning approaches.\n",
    "\n",
    "For this lesson, we will use a large convnet trained on the **ImageNet** dataset which consists of 1.4 million labeled images of animals spanning 1,000 different classes. We will also use VGG16 architecture which is a simple and widely used convnet architecture for ImageNet.\n",
    "\n",
    "There are two ways to use a pretrained network: **feature extraction** and **fine-tuning**. We will cover both in following sections.\n",
    "\n",
    "### 5.3.1 Feature extraction\n",
    "Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier that is trained from scratch.\n",
    "\n",
    "To review, convnets used to image classification comprise two parts: they begin with a series of pooling and convolution layers, and end with a densely connected classifier. The first portion is called the *convolutional base*. Feature extraction consists of taking the convolutional base of a previously trained network, running new data through it, and trainined a new classifier on top of the output.\n",
    "\n",
    "![feature extraction](images/5_3_1_featextr.jpg)\n",
    "\n",
    "One should refrain from reusing the densely connected classifier portion of the pretrained model because the representations learned by the convolutional base are likely to be more generic and reusable. The representations learned from the densely connected classifier will be specific to the set of classes on which the model was trained, and generally not broad or flexible enough to be reused to train a new model. In addition, densely connected layers no longer contain any information about *where* objects are located in an input image, which can also be burdensome.\n",
    "\n",
    "The level of generality of the representation extracted by convolutional layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps such as visual edges, colors, and textures. Layers that are higher up extract more abstract concepts such as \"cat ear\" or \"dog eye\".\n",
    "\n",
    "Because the ImageNet class set contains multiple dog and cat classes, it's likely to be beneficial to reuse the information contained in the densely connected layers of the original model, but we'll choose not to for the sake of learning the more common practice of only using the convolutional base (specifically the VGG16 architecture in this case).\n",
    "\n",
    "Let's instantiate the VGG16 model, which comes prepackaged with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58826752/58889256 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the three arguments we passed to VGG16:\n",
    " - **`weights`** specifies the weight checkpoint from which to initialize the model.\n",
    " - **`include_top`** refers to including the densely connected classifier on top of the network.\n",
    " - **`input_shape`** is the shape of the image tensors that we will feed the network, but this is an optional argument. \n",
    " \n",
    "Let's view the detail of the VGG16 convolutional base architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final feature map has shape (4, 4, 512) which is where we will stick a densely connected classifier.\n",
    "\n",
    "There are two ways to proceed from this point:\n",
    " 1. Running the convolutional base over the dataset, recording its output to a Numpy array on disk, and then using this data as input to a standalone, densely connected classifier. This solution is fast and cheap to run, because it only requires running the convolutional base once for every input image, however, this method prevents you from using data augmentation.\n",
    " 2. Extending the model you have (`conv_base`) by adding `Dense` layers on top, and running the whole thing end to end on the input data. This will allow data augmentation, but this technique is far more expensive than the first method.\n",
    " \n",
    "**FAST FEATURE EXTRACTION WITHOUT DATA AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = '/Volumes/RobStorage/Desktop/Python_Practice/DL_with_Python/data/cats_and_dogs_small'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(directory, target_size=(150, 150),\n",
    "                                            batch_size=batch_size, class_mode='binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the extracted features are currently of shape (`samples, 4, 4, 512`), we'll feed them through a densely connected classifier. To do this, we must first flatten them to (`samples, 8192`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can define our densely connected classifier (note the use of dropout for regularization) and train it on the data and labels we just recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.6047 - acc: 0.6635 - val_loss: 0.4498 - val_acc: 0.8030\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.4222 - acc: 0.8010 - val_loss: 0.3665 - val_acc: 0.8430\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.3607 - acc: 0.8475 - val_loss: 0.3242 - val_acc: 0.8630\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.3201 - acc: 0.8695 - val_loss: 0.2989 - val_acc: 0.8840\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.2907 - acc: 0.8840 - val_loss: 0.2834 - val_acc: 0.8910\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.2638 - acc: 0.8970 - val_loss: 0.2736 - val_acc: 0.8890\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.2481 - acc: 0.9060 - val_loss: 0.2667 - val_acc: 0.8910\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.2277 - acc: 0.9220 - val_loss: 0.2572 - val_acc: 0.8950\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.2299 - acc: 0.9150 - val_loss: 0.2542 - val_acc: 0.8970\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.2063 - acc: 0.9180 - val_loss: 0.2482 - val_acc: 0.9010\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.2004 - acc: 0.9210 - val_loss: 0.2444 - val_acc: 0.9010\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1850 - acc: 0.9270 - val_loss: 0.2491 - val_acc: 0.8990\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.1818 - acc: 0.9295 - val_loss: 0.2538 - val_acc: 0.8930\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.1800 - acc: 0.9295 - val_loss: 0.2378 - val_acc: 0.89800.9\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1613 - acc: 0.9450 - val_loss: 0.2367 - val_acc: 0.9010\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.1622 - acc: 0.9405 - val_loss: 0.2349 - val_acc: 0.9000\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1483 - acc: 0.9475 - val_loss: 0.2355 - val_acc: 0.9040\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1497 - acc: 0.9510 - val_loss: 0.2340 - val_acc: 0.9000\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1476 - acc: 0.9490 - val_loss: 0.2403 - val_acc: 0.9000\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1385 - acc: 0.9530 - val_loss: 0.2361 - val_acc: 0.9020\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1297 - acc: 0.9555 - val_loss: 0.2324 - val_acc: 0.8990\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1302 - acc: 0.9560 - val_loss: 0.2312 - val_acc: 0.9000\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1221 - acc: 0.9600 - val_loss: 0.2302 - val_acc: 0.9000\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1180 - acc: 0.9625 - val_loss: 0.2357 - val_acc: 0.9010\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1134 - acc: 0.9630 - val_loss: 0.2316 - val_acc: 0.9000\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 4s - loss: 0.1087 - acc: 0.9670 - val_loss: 0.2312 - val_acc: 0.9010\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.1000 - acc: 0.9690 - val_loss: 0.2358 - val_acc: 0.9010\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.0997 - acc: 0.9685 - val_loss: 0.2379 - val_acc: 0.9030\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.0979 - acc: 0.9705 - val_loss: 0.2388 - val_acc: 0.9050\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 5s - loss: 0.0909 - acc: 0.9680 - val_loss: 0.2370 - val_acc: 0.9020\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels, epochs=30, batch_size=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we only had to deal with two `Dense` layers, the training should be extremely fast.\n",
    "\n",
    "Let's take a look at the loss and accuracy curves during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/CIiBUbBLyJSYP4/+Wlni0NzLEoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a43b0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPNoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1a963890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reach a validation accuracy of about 90% before it begins to plateau, which is significantly better than the model we trained from scratch which attained an accuracy of 82%. Unfortunately, the plots also indicate that we are overfitting almost from the start, despite using dropout. The reason for that is because this technique doesn't use data augmentation, which is critical for preventing overfitting with small image datasets.\n",
    "\n",
    "**FEATURE EXTRACTION WITH DATA AUGMENTATION**\n",
    "Now, we will review the second technique for doing feature extraction, which is much slower and more expensive, but allows us to use data augmentation during training.\n",
    "\n",
    " - **Note**: training this model on a CPU is nearly impossible. Only attempt to replicate code if you have access to a GPU.\n",
    " \n",
    "Because models behave just like layers, we can add a model (like `conv_base`) to a `Sequential` model just like we would add a layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 16,812,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional base of VGG16 has 14,714,688 parameters, which is pretty big. The classifier we're adding on top has 2 million parameters.\n",
    "\n",
    "Before compiling and training the model, it's important to freeze the convolutional base. **Freezing** a layer or set of layers means preventing their weights from being updated during training. If we don't do this, representations that were previously learned will be modified during training, which would effectively destroy the network.\n",
    "\n",
    "In Keras, we freeze a network by setting its `trainable` attribute to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is the number of trainable weights before freezing the conv base:', 30)\n",
      "('This is the number of trainable weights after freezing the conv base:', 4)\n"
     ]
    }
   ],
   "source": [
    "print('This is the number of trainable weights '\n",
    "         'before freezing the conv base:', len(model.trainable_weights))\n",
    "conv_base.trainable = False\n",
    "print('This is the number of trainable weights '\n",
    "          'after freezing the conv base:', len(model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, only the weights from the two `Dense` layers that we added will be trained. That's a total of four weight tensors: two per layer (the main weight matrix and the bias vector). In order for these changes to take effect, we must first compile the model. If we ever modify weight trainability after compilation, then we should recompile the model, or these changes will be ignored.\n",
    "\n",
    "Now to start training the model, with the same data-augmentation configuration from an earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "# Validation data shouldn't be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)                \n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,  # target directory                                          \n",
    "        target_size=(150, 150), # resizes images to 150 x 150                               \n",
    "        batch_size=20,\n",
    "        class_mode='binary') # because we use binary_crossentropy loss, we need binary labels                                \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to plot the results!\n",
    "\n",
    "![pretrained1](images/5_3_1_pretrain1.jpg)\n",
    "![pretrained2](images/5_3_1_pretrain2.jpg)\n",
    "\n",
    "Wow. We were able to reach a validation accuracy of about 96%. This is a major improvement.\n",
    "\n",
    "### 5.3.2 Fine-tuning\n",
    "Another widely used technique for model reuse, complementary to feature extraction, is **fine-tuning**. Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model and these top layers. It's called *fine-tuning* because it slightly adjusts the more abstract representations of the model being reused, in order to make them more relevant for the problem at hand.\n",
    "\n",
    "![finetuning](images/5_3_2_finetuning.jpg)\n",
    "\n",
    "As previously stated, it's necessary to freeze the convolution base of VGG16 in order to be able to train a randomly initialized classifier on top. It's only possible to fine-tune the top layers of the convolutional base once the classifier on top has already been trained. If the classifier isn't already trained, then the error signal propagating through the network during training will be too large, and the representations previously learned by the layers being fine-tuned will be destroyed.\n",
    "\n",
    "Steps for fine-tuning:\n",
    " 1. **Add the custom network on top of an already-trained base network.**\n",
    " 2. **Freeze the base network.**\n",
    " 3. **Train the part that was added.**\n",
    " 4. **Unfreeze some layers in the base network.**\n",
    " 5. **Jointly train these layers and the part that was added.**\n",
    " \n",
    "We already completed the first three steps, so let's proceed to step 4 and unfreeze `conv_base` and then freeze individual layers inside of it.\n",
    "\n",
    "Let's take a look at convolutional base again to get a better idea of what we are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine-tune the last three convolutional layers, so all layers up to **`block4_pool`** should be frozen, and the layers **`block5_conv1, block5_conv2,`** and **`block_conv3`** should be trainable.\n",
    "\n",
    "***Why not fine-tune more layers? Why not fine-tune the entire convolutional base?***\n",
    "\n",
    "We won't fine-tune more layers for two reasons:\n",
    " 1. Earlier layers in the convolutional base encode more-generic, reusable features, whereas layers higher up encode more-specialized features, which are the ones that we need to repurpose for our new problem.\n",
    " 2. The more parameters we train, the greater the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin fine-tuning the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results!\n",
    "\n",
    "![ft1](images/5_3_2_ft1.jpg)\n",
    "![ft2](images/5_3_2_ft2.jpg)\n",
    "\n",
    "These curves look pretty noisy. To make them more readable, we can smooth them by replacing every loss and accuracy with exponential moving averages of these quantities. Here is the function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.8):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "plt.plot(epochs, smooth_curve(acc), 'bo', label='Smoothed training acc')\n",
    "plt.plot(epochs, smooth_curve(val_acc), 'b', label='Smoothed validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, smooth_curve(loss), 'bo', label='Smoothed training loss')\n",
    "plt.plot(epochs, smooth_curve(val_loss), 'b', label='Smoothed validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ft3](images/5_3_2_ft3.jpg)\n",
    "![ft4](images/5_3_2_ft4.jpg)\n",
    "\n",
    "The validation accuracy curve looks much cleaner this time! Our accuracy jumped from (an already strong) 96% to **above 97%**!\n",
    "\n",
    "On the other hand, the loss curve doesn't show any real improvement. So, how could accuracy stay stable or improve if the loss isn't decreasing? This is because what we display is an average of pointwise loss values; but what matters for accuracy is the dirstribution of the loss values, not their average, because accuracy is the result of a binary thresholding of the class probability predicted by the model. The model may still be improving even if this isn't reflected in the average loss.\n",
    "\n",
    "We can now finally evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_loss, test_acc = model.evaluate_generator(test_generator, steps=50)\n",
    "print('test acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was able to produce a **test accuracy of 97%!** In the original Kaggle competition around this dataset, this would have been one of the top results. By using modern deep-learning techniques, we managed to reach this result only using a small fraction of the training data available (~10%). Imagine if we were to train on all 20,000 samples!\n",
    "\n",
    "### 5.3.3 Wrapping up\n",
    "Here are some key takeaways from the past two sections:\n",
    "\n",
    " - Convnets are the best type of machine-learning models for computer-vision tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.\n",
    " - On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.\n",
    " - It’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\n",
    " - As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing model. This pushes performance a bit further.\n",
    "Now you have a solid set of tools for dealing with image-classification problems—in particular with small datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
