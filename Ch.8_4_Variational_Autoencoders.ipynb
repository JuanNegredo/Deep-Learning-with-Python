{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 Generating images with variational autoencoders\n",
    "Sampling from a collection of images to create entirely new images or edit existing ones is currently the most popular and successful application of creative AI. In this section, we’ll look at concepts pertaining to image generation, alongside implementation details relative to the two main techniques in this domain: variational autoencoders (VAEs) and generative adversarial networks (GANs). Creating new images is not the only thing these techniques are capable of producing — one could also create sound, music, or even text.\n",
    "\n",
    "## 8.4.1 Sampling from latent spaces of images\n",
    "The main concept of image generation is to develop a low-dimensional latent space of representations where any point can be mapped to a realistic-looking image. The module capable of realizing this mapping, taking as input a latent point and outputting an image (a grid of pixels), is called a generator (in the case of GANs) or a decoder (in the case of VAEs). Once such a latent space has been developed, you can sample points from it, and generate images that have never been seen before.\n",
    "\n",
    "![latent](images/8_4_1_latent.jpg)\n",
    "\n",
    "GANs and VAEs are two different strategies for learning such latent spaces of image representations. Variational Autoencoders are great for learning latent spaces that are well structured. General Adversarial Networks generate images that can potentially be highly realistic, but the latent space they come from may not have as much structure and continuity. An example of faces generated using VAEs is shown below.\n",
    "\n",
    "![vae](images/8_4_1_vae.png)\n",
    "\n",
    "## 8.4.2 Concept vectors for image editing\n",
    "Given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. In a latent space of images of faces, for instance, there may be a smile vector, such that if latent point z is the embedded representation of a certain face, then latent point z + s is the embedded representation of the same face, smiling. In the case of faces, you may discover vectors for adding sunglasses to a face, removing glasses, turning a male face into a female face, and so on. The image below is an example of a smile vector using VAEs trained on a dataset of faces of celebrities (the CelebA dataset).\n",
    "\n",
    "![smile](images/8_4_2_smile.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 8.4.3 Variational Autoencoders\n",
    "Variational autoencoders are a kind of generative model that’s is appropriate for the task of image editing via concept vectors. They’re a modern take on autoencoders and mixes ideas from deep learning with Bayesian inference.\n",
    "\n",
    "A classical image autoencoder takes an image, maps it to a latent vector space via an encoder module, and then decodes it back to an output with the same dimensions as the original image, via a decoder module (see figure 8.12). An autoencoder is made of two components. The encoder brings the data from a high dimensional input to a bottleneck layer, where the number of neurons is the smallest. Then, the decoder takes this encoded input and converts it back to the original input shape — in our case an image. The latent space is the space in which the data lies in the bottleneck layer.\n",
    "\n",
    "So...what the hell does this mean? Let's an encoder receives a color image. To convert the color image into numerical values, the encoder must convert the image to grayscale - where 0 is white and 100 is black with varying shades of gray in between. You must be thinking, *but a color image isn't on a grayscale*, and you are right, but we have a trick for that shown below. The encoder splits the RGB images into 3 new images for each shade - Red, Green, and Blue.\n",
    "\n",
    "[rgb_example](images/8_4_3_rgb_example.png)\n",
    "\n",
    "Once the encoder has converted an input image into a bunch of numerical values, it then compresses the high-dimensional image into a 1D-vector which is essentially a long, single file line of numbers - each number representing a grayscale value of the pixels shade. This long, single file line of numbers is known as the image **latent space**. Here's a good visualization below.\n",
    "\n",
    "[latent2](images/8_4_3_later.png)\n",
    "\n",
    "The encoder is then trained using the exact same input images as its target output, meaning the autoencoder learns to reconstruct the original inputs - i.e. recreate the original image. By imposing various constraints on the code (the output of the encoder), we can get the autoencoder to learn interesting latent representations of the data. Most commonly, we will constrain the code to be low-dimensional and sparse (mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.\n",
    "\n",
    "![autoencoder](images/8_4_3_autoencoder.jpg)\n",
    "\n",
    "In practice, such classical autoencoders don’t lead to particularly great results. They’re not much good at compression, either. For these reasons, they have largely fallen out of fashion. VAEs, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a powerful tool for image generation.\n",
    "\n",
    "A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means you’re assuming the input image has been generated by a statistical process, and that the randomness of this process should be taken into account during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input (see figure 8.13). The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere: every point sampled in the latent space is decoded to a valid output.\n",
    "\n",
    "![vae](images/8_4_3_vae.jpg)\n",
    "\n",
    "In technical terms, here’s how a VAE works:\n",
    "\n",
    " 1. An encoder module turns the input samples input_img into two parameters in a latent space of representations, z_mean and z_log_variance.\n",
    " 2. You randomly sample a point z from the latent normal distribution that’s assumed to generate the input image, via z = z_mean + exp(z_log_variance) * epsilon, where epsilon is a random tensor of small values.\n",
    " 3. A decoder module maps this point in the latent space back to the original input image.\n",
    "\n",
    "Because epsilon is random, the process ensures that every point that’s close to the latent location where you encoded input_img (z-mean) can be decoded to something similar to input_img, thus forcing the latent space to be continuously meaningful. Any two close points in the latent space will decode to highly similar images. Continuity, combined with the low dimensionality of the latent space, forces every direction in the latent space to encode a meaningful axis of variation of the data, making the latent space very structured and thus highly suitable to manipulation via concept vectors.\n",
    "\n",
    "The parameters of a VAE are trained via two loss functions: a reconstruction loss that forces the decoded samples to match the initial inputs, and a regularization loss that helps learn well-formed latent spaces and reduce overfitting to the training data. Let’s quickly go over a Keras implementation of a VAE. Schematically, it looks like this:\n",
    "\n",
    "`z_mean, z_log_variance = encoder(input_img)`\n",
    "\n",
    "`z = z_mean + exp(z_log_variance) * epsilon`\n",
    "\n",
    "`reconstructed_img = decoder(z)`\n",
    "\n",
    "`model = Model(input_img, reconstructed_img)`\n",
    "\n",
    "You can then train the model using the reconstruction loss and the regularization loss.\n",
    "\n",
    "The following listing shows the encoder network you’ll use, mapping images to the parameters of a probability distribution over the latent space. It’s a simple convnet that maps the input image x to two vectors, z_mean and z_log_var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2\n",
    "\n",
    "input_img = keras.Input(shape=img_shape)\n",
    "\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the code for using z_mean and z_log_var, the parameters of the statistical distribution assumed to have produced input_img, to generate a latent space point z. Here, you wrap some arbitrary code (built on top of Keras backend primitives) into a Lambda layer. In Keras, everything needs to be a layer, so code that isn’t part of a built-in layer should be wrapped in a Lambda (or in a custom layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following listing shows the decoder implementation. You reshape the vector z to the dimensions of an image and then use a few convolution layers to obtain a final image output that has the same dimensions as the original input_img."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
    "\n",
    "x = layers.Dense(np.prod(shape_before_flattening[1:]), activation='relu')(decoder_input)\n",
    "\n",
    "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
    "x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
    "x = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
    "\n",
    "decoder = Model(decoder_input, x)\n",
    "\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual loss of a VAE doesn’t fit the traditional expectation of a sample-wise function of the form loss(input, target). Thus, you’ll set up the loss by writing a custom layer that internally uses the built-in add_loss layer method to create an arbitrary loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        kl_loss = -5e-4 * K.mean(\n",
    "            1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "y = CustomVariationalLayer()([input_img, z_decoded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you’re ready to instantiate and train the model. Because the loss is taken care of in the custom layer, you don’t specify an external loss at compile time (loss=None), which in turn means you won’t pass target data during training (as you can see, you only pass x_train to the model in fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 28, 28, 32)    320         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 14, 14, 64)    18496       conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 14, 14, 64)    36928       conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 14, 14, 64)    36928       conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 12544)         0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            401440      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             66          dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             66          dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 2)             0           dense_2[0][0]                    \n",
      "                                                                   dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "model_1 (Model)                  (None, 28, 28, 1)     56385       lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cust [(None, 28, 28, 1), ( 0           input_1[0][0]                    \n",
      "                                                                   model_1[1][0]                    \n",
      "====================================================================================================\n",
      "Total params: 550,629\n",
      "Trainable params: 550,629\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 664s - loss: 534551.1572 - val_loss: 0.1986\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 682s - loss: 0.1951 - val_loss: 0.1920\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 747s - loss: 0.1910 - val_loss: 0.1887\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 753s - loss: 0.1895 - val_loss: 0.1913\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 756s - loss: nan - val_loss: nan\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 645s - loss: nan - val_loss: nan\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 630s - loss: nan - val_loss: nan\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 637s - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 632s - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 626s - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2dab6b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae = Model(input_img, y)\n",
    "vae.compile(optimizer='rmsprop', loss=None)\n",
    "vae.summary()\n",
    "\n",
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "\n",
    "vae.fit(x=x_train, y=None, shuffle=True, epochs=10, batch_size=batch_size, validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once such a model is trained—on MNIST, in this case—you can use the decoder network to turn arbitrary latent space vectors into images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAJCCAYAAADQsoPKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAgAY/D/aXE4dIiTihQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1ed56550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "n = 15\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
    "        x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of sampled digits below shows a completely continuous distribution of the different digit classes, with one digit morphing into another as you follow a path through latent space. Specific directions in this space have a meaning: for example, there’s a direction for “four-ness,” “one-ness,” and so on.\n",
    "\n",
    "![mnist](images/8_4_3_mnist.png)\n",
    "\n",
    "In the next section, we’ll cover the other major tool for generating artificial images: generative adversarial networks (GANs).\n",
    "\n",
    "8.4.4. Wrapping up\n",
    " - Image generation with deep learning is done by learning latent spaces that capture statistical information about a dataset of images. By sampling and decoding points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs.\n",
    " - VAEs result in highly structured, continuous latent representations. For this reason, they work well for doing all sorts of image editing in latent space: face swapping, turning a frowning face into a smiling face, and so on. They also work nicely for doing latent-space-based animations, such as animating a walk along a cross section of the latent space, showing a starting image slowly morphing into different images in a continuous way.\n",
    " - GANs enable the generation of realistic single-frame images but may not induce latent spaces with solid structure and high continuity.\n",
    " - Most successful practical applications I have seen with images rely on VAEs, but GANs are extremely popular in the world of academic research—at least, circa 2016–2017. You’ll find out how they work and how to implement one in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
