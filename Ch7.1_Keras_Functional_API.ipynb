{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.7 Advanced Deep Learning Best Practices\n",
    "## 7.1 Going beyond the Sequential model: the Keras functional API\n",
    "\n",
    "Up to this point, all neural networks introduced in these studies have been implemented using the **`Sequential`** model, which makes the assumption that the network has exactly one input and one output, and consists of a linear stack of layers. However, some networks require several indpendent inputs, others require multiple outputs, and some have internal branching between layers that makes them look like graphs of layers rather than a linear stack of layers.\n",
    "\n",
    "Some tasks require *multimodal* inputs: the merge data coming from different input sources, processing each type of data using different kinds of neural layers. Picture a deep learning model that is trying to predict the most likely market price of a second-hand piece of clothing, using only user-provided metadata information on the item (brand, age, etc.), a text description, and a photo.\n",
    " - If we only had the metadata, we could one-hot encode it and use a densely connected network to predict the price.\n",
    " - If we only had the text description, we could use an RNN of a 1D CNN.\n",
    " - If we only had the photo, we could use a 2D CNN.\n",
    " \n",
    "But how can we use all three at the same time? A good way is to *jointly* learn a more accurate model of the data by using a model that can see all available input modalities simultaneously: a model with three branches.\n",
    "\n",
    "![multi input model](images/7_1_1_multiinput.jpg)\n",
    "\n",
    "Similarly, some tasks need to predict multiple target attributes of input data. Given the text of a book, we might want to automatically classify it by genre but also predict the date it was written. We could train two separate models, but genre and date written are not statistically independent, and we could build a better model that learns jointly to predict genre and date at the same time.\n",
    "\n",
    "![multi output model](images/7_1_1_multioutput.jpg)\n",
    "\n",
    "There even exist crazier, more complex networks structured as acyclic graphs. Here is a picture of an Inception model developed by Google.\n",
    "\n",
    "![inception](images/7_1_1_inception.jpg)\n",
    "\n",
    "## 7.1.1 Introduction to the functional API\n",
    "\n",
    "With the functional API, we directly manipulate tensors, and use layers as functions that take tensors and return tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, layers\n",
    "\n",
    "input_tensor = Input(shape=(32,)) # a tensor\n",
    "\n",
    "dense = layers.Dense(32, activation='relu') # layer is a function\n",
    "\n",
    "output_tensor = dense(input_tensor) # layer may be called on a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a minimal example that shows a simple **Sequential** model side-by-side with its equivalent in the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 3,466\n",
      "Trainable params: 3,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "seq_model = Sequential() # already know about this model\n",
    "seq_model.add(layers.Dense(32, activation='relu', input_shape=(64,))) #1st hidden layer\n",
    "seq_model.add(layers.Dense(32, activation='relu')) # 2nd hidden layer\n",
    "seq_model.add(layers.Dense(10, activation='softmax')) # output layer\n",
    "\n",
    "# Below is the functional API equivalent as the model above\n",
    "input_tensor = Input(shape=(64,)) # input layer\n",
    "x = layers.Dense(32, activation='relu')(input_tensor) # 1st hidden layer\n",
    "x = layers.Dense(32, activation='relu')(x) # 2nd hidden layer\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x) #output layer\n",
    "\n",
    "# Model class turns an input tensor and output tensor into a model\n",
    "model = Model(input_tensor, output_tensor)\n",
    "\n",
    "# let's look at it!\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, Keras retrieves every layer involved in going from `input_tensor` to `output_tensor`, bringing them together into a graph-like data structure - a **Model**. The reason it works is because the `output_tensor` was obtained by repeatedly transforming the `input_tensor`. If we tried to build a model from inputs and outputs that weren't related, we'd get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 64), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-54197a8d0ec3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0munrelated_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbad_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrelated_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1788\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1791\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_2:0\", shape=(?, 64), dtype=float32) at layer \"input_2\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "unrelated_input = Input(shape=(32,))\n",
    "bad_model = model = Model(unrelated_input, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This error tells us that Keras couldn't reach `input_1` from the provided output tensor.\n",
    "\n",
    "When it comes to compiling, training, or evaluating such an instance of `Model`, the API is the same as `Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.5303     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4694     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4584     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4521     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4476     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4434     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4413     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4387     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4367     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 11.4349     \n",
      "  32/1000 [..............................] - ETA: 3s"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy') # compiles the model\n",
    "\n",
    "# generate dummy data to train on\n",
    "import numpy as np\n",
    "x_train = np.random.random((1000, 64))\n",
    "y_train = np.random.random((1000, 10))\n",
    "\n",
    "# train the model for 10 epochs\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 Multi-input models\n",
    "\n",
    "The functional API can be used to build models that have multiple inputs. Typically, such models merge their different input branches using a layer that can combine several tensors: by adding them, concatenating them, and so on. Let's look at a very simple example of a multi-input model: a question-answering model.\n",
    "\n",
    "A typical question-answering model has two inputs: a natural-language question and a text snippet (such as a news article) providing information to be used for answering the question. The model must then produce an answer: in the simplest possible setup, this is a one-word answer obtained via a softmax over some predefined vocabulary.\n",
    "\n",
    "![question](images/7_1_2_question.jpg)\n",
    "\n",
    "Here is an example of how we can build this model using the functional API. We set up two independent branches, encoding the text input and the question input as representation vectors, then we concatenate these vectors, and add a softmax classifier on top of the concatenated representations.\n",
    "\n",
    "**FUNCTIONAL API IMPLEMENTATION OF A TWO-INPUT QUESTION-ANSWERING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_vocabulary_size = 10000\n",
    "question_vocabulary_size = 10000\n",
    "answer_vocabulary_size = 500\n",
    "\n",
    "# text input is a variable-length sequence of integers\n",
    "text_input = Input(shape=(None,), dtype='int32', name='text')\n",
    "\n",
    "# embed the inputs into a sequence of vectors of size 64\n",
    "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input)\n",
    "\n",
    "# encodes the vectors in a single vector via an LSTM\n",
    "encoded_text = layers.LSTM(32)(embedded_text)\n",
    "\n",
    "# same process with different layer instances for the question\n",
    "question_input = Input(shape=(None,), dtype='int32', name='question')\n",
    "\n",
    "embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
    "encoded_question = layers.LSTM(16)(embedded_question)\n",
    "\n",
    "# concatenate the encoded question and encoded text\n",
    "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1)\n",
    "\n",
    "# add a softmax classifier on top\n",
    "answer = layers.Dense(answer_vocabulary_size, activation='softmax')(concatenated)\n",
    "\n",
    "# model instantiation. Specify the two inputs and output\n",
    "model = Model([text_input, question_input], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we train this two-input model? We can feed the model a list of Numpy arrays as inputs, or we can feed it a dictionary that maps input names to Numpy arrays (only an option if we give names to inputs).\n",
    "\n",
    "**FEEDING DATA TO A MULTI-INPUT MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1000\n",
    "max_length = 100\n",
    "\n",
    "# generate dummy Numpy data\n",
    "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length))\n",
    "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
    "\n",
    "# answers are one-hot encoded, not integers\n",
    "answers = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit using a list of inputs\n",
    "model.fit([text, question], answers, epochs=10, batch_size=128)\n",
    "\n",
    "# fit using a dictionary of inputs (only if inputs are named)\n",
    "#model.fit({'text': text, 'question': question}, answers, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Multi-output models\n",
    "\n",
    "In the same way as above, we can use the functional API to build models with multiple *outputs* (or multiple *heads*). A simple example is a network that attempts to simultaneously predict different properties of the data, such as a network that takes a series of social media posts from one person as inputs and tries to predict attributes of that person, such as age, gender, and income level.\n",
    "\n",
    "![multi output](images/7_1_3_multioutput.jpg)\n",
    "\n",
    "**FUNCTIONAL API IMPLEMENTATION OF A THREE-OUTPUT MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "vocabulary_size = 50000\n",
    "num_income_groups = 10\n",
    "\n",
    "posts_input = Input(shape=(None,), dtype='int32', name='posts')\n",
    "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
    "x = layers.Conv1D(128, 5, activation='relu')(embedded_posts)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.Conv1D(256, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "age_prediction = layers.Dense(1, name='age')(x) \n",
    "income_prediction = layers.Dense(num_income_groups,\n",
    "                                 activation='softmax',\n",
    "                                 name='income')(x)\n",
    "gender_prediction = layers.Dense(1, activation='sigmoid', name='gender')(x)\n",
    "\n",
    "model = Model(posts_input,\n",
    "              [age_prediction, income_prediction, gender_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training such a model requires the ability to specify different loss functions for different heads of the network: age prediction is a scalar regression task, but gender is a binary classification task, requiring a different training procedure. But because gradient descent requires us to minimize a scalar, we must combine these losses into a single value in order to train the model. The simplest way to combine different losses is to sum them all.\n",
    "\n",
    "**COMPILATION OPTIONS OF A MULTI-OUTPUT MODEL: MULTIPLE LOSSES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very imbalanced loss contributions will cause the model representations to be optimized preferentially for the task with the largest individual loss, at the expense of other tasks. To remedy this, we can assign different levels of importance to the loss values in their contribution to the final loss. For instance, the MSE loss used for the age-regression typically takes a value around 3-5, whereas the cross-entropy loss used for gender-classification can be as low as 0.1. In this situation, we can assign a weight of 10 to the crossentropy loss and a weight of 0.25 to the MSE loss.\n",
    "\n",
    "**COMPILATION OPTIONS OF A MULTI-OUTPUT MODEL: LOSS WEIGHTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', \n",
    "              loss=['mse', 'categorical_crossentropy', 'binary_crossentropy'],\n",
    "              loss_weights=[0.25, 1., 10.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feeding data to a multi-output model\n",
    "model.fit(posts, [age_targets, income_targets, gender_targets],\n",
    "          epochs=10, batch_size=64)\n",
    "\n",
    "model.fit(posts, {'age': age_targets,\n",
    "                  'income': income_targets, \n",
    "                  'gender': gender_targets}, \n",
    "          epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.4 Directed acyclic graphs of layers\n",
    "\n",
    "With the functional API, not only can we build models with multiple inputs and multiple outputs, but we can also implement networks with a complex internal topology. Neural networks in Keras are allowed to be arbitrary *directed acyclic graphs* of layers. The qualifier **acyclic** is important: these graphs can’t have cycles. It’s impossible for a tensor x to become the input of one of the layers that generated x. The only processing loops that are allowed (that is, recurrent connections) are those internal to recurrent layers.\n",
    "\n",
    "Several common neural-network components are implemented as graphs. Two notable ones are Inception modules and residual connections. To better understand how the functional API can be used to build graphs of layers, let’s take a look at how we can implement both of them in Keras.\n",
    "\n",
    "### Inception modules\n",
    "*Inception* is a popular type of network architecture for convolutional neural networks. It consists of a stack of modules that themselves look like small independent networks, split into several parallel branches. The most basic form of an Inception module has three to four branches starting with a 1 × 1 convolution, followed by a 3 × 3 convolution, and ending with the concatenation of the resulting features. This setup helps the network separately learn spatial features and channel-wise features, which is more efficient than learning them jointly. More-complex versions of an *Inception* module are also possible, typically involving pooling operations, different spatial convolution sizes (for example, 5 × 5 instead of 3 × 3 on some branches), and branches without a spatial convolution (only a 1 × 1 convolution). An example of such a module, taken from Inception V3, is shown below.\n",
    "\n",
    "![inception](images/7_1_4_inception.jpg)\n",
    "\n",
    "**THE PURPOSE OF 1x1 CONVOLUTIONS**\n",
    "\n",
    "We already know that convolutions extract spatial patches around every tile in an input tensor and apply the same transformation to each patch. An edge case is when the patches extracted consist of a single tile. The convolution operation then becomes equivalent to running each tile vector through a Dense layer: it will compute features that mix together information from the channels of the input tensor, but it won’t mix information across space (because it’s looking at one tile at a time). Such 1 × 1 convolutions (also called pointwise convolutions) are featured in *Inception* modules, where they contribute to factoring out channel-wise feature learning and space-wise feature learning—a reasonable thing to do if we assume that each channel is highly autocorrelated across space, but different channels may not be highly correlated with each other.\n",
    "\n",
    "Here's how to implement the module featured in the image above using the functional API. Note this example assumes the existence of a 4D input tensor x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# every branch has the same stride value. This keeps all branch outputs the same size.\n",
    "branch_a = layers.Conv2D(128, 1, activation='relu', strides=2)(x) \n",
    "branch_b = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_b = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_b)\n",
    "\n",
    "# striding occurs in the average pooling layer\n",
    "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
    "branch_c = layers.Conv2D(128, 3, activation='relu')(branch_c)\n",
    "\n",
    "branch_d = layers.Conv2D(128, 1, activation='relu')(x)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu')(branch_d)\n",
    "branch_d = layers.Conv2D(128, 3, activation='relu', strides=2)(branch_d)\n",
    "\n",
    "# concatenates the branch outputs to obtain the module output\n",
    "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the full Inception V3 architecture is available in Keras as `keras.applications.inception_v3.InceptionV3`, including weights pretrained on the ImageNet dataset. Another closely related model available as part of the Keras applications module is **Xception**. Xception, which stands for extreme inception, is a convnet architecture loosely inspired by Inception. It takes the idea of separating the learning of channel-wise and space-wise features to its logical extreme, and replaces Inception modules with depthwise separable convolutions consisting of a depthwise convolution (a spatial convolution where every input channel is handled separately) followed by a pointwise convolution (a 1 × 1 convolution)—effectively, an extreme form of an Inception module, where spatial features and channel-wise features are fully separated. Xception has roughly the same number of parameters as Inception V3, but it shows better runtime performance and higher accuracy on ImageNet as well as other large-scale datasets, due to a more efficient use of model parameters.\n",
    "\n",
    "### Residual Connections\n",
    "*Residual connections* are a common graph-like network component found in many post-2015 network architectures, including Xception. They tackle two common problems that plague any large-scale deep-learning model: vanishing gradients and representational bottlenecks. In general, adding residual connections to any model that has more than 10 layers is likely to be beneficial.\n",
    "\n",
    "A residual connection consists of making the output of an earlier layer available as input to a later layer, effectively creating a shortcut in a sequential network. Rather than being concatenated to the later activation, the earlier output is summed with the later activation, which assumes that both activations are the same size. If they’re different sizes, we can use a linear transformation to reshape the earlier activation into the target shape (for example, a Dense layer without an activation or, for convolutional feature maps, a 1 × 1 convolution without an activation).\n",
    "\n",
    "Here’s how to implement a residual connection in Keras when the feature-map sizes are the same, using identity residual connections. This example assumes the existence of a 4D input tensor x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = ...\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)    \n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "\n",
    "y = layers.add([y, x])                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following implements a residual connection when the feature-map sizes differ, using a linear residual connection (again, assuming the existence of a 4D input tensor x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "x = ...\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.MaxPooling2D(2, strides=2)(y)\n",
    "\n",
    "residual = layers.Conv2D(128, 1, strides=2, padding='same')(x)       \n",
    "\n",
    "y = layers.add([y, residual])                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REPRESENTATIONAL BOTTLENECKS IN DEEP LEARNING**\n",
    "\n",
    "In a Sequential model, each successive representation layer is built on top of the previous one, which means it only has access to information contained in the activation of the previous layer. If one layer is too small (for example, it has features that are too low-dimensional), then the model will be constrained by how much information can be crammed into the activations of this layer.\n",
    "\n",
    "You can grasp this concept with a signal-processing analogy: if you have an audio-processing pipeline that consists of a series of operations, each of which takes as input the output of the previous operation, then if one operation crops your signal to a low-frequency range (for example, 0–15 kHz), the operations downstream will never be able to recover the dropped frequencies. Any loss of information is permanent. Residual connections, by reinjecting earlier information downstream, partially solve this issue for deep-learning models.\n",
    "\n",
    "**VANISHING GRADIENTS IN DEEP LEARNING**\n",
    "\n",
    "Backpropagation, the master algorithm used to train deep neural networks, works by propagating a feedback signal from the output loss down to earlier layers. If this feedback signal has to be propagated through a deep stack of layers, the signal may become tenuous or even be lost entirely, rendering the network untrainable. This issue is known as vanishing gradients.\n",
    "\n",
    "This problem occurs both with deep networks and with recurrent networks over very long sequences—in both cases, a feedback signal must be propagated through a long series of operations. We’re already familiar with the solution that the LSTM layer uses to address this problem in recurrent networks: it introduces a carry track that propagates information parallel to the main processing track. Residual connections work in a similar way in feedforward deep networks, but they’re even simpler: they introduce a purely linear information carry track parallel to the main layer stack, thus helping to propagate gradients through arbitrarily deep stacks of layers.\n",
    "\n",
    "## 7.1.5 Layer weight sharing\n",
    "One more important feature of the functional API is the ability to reuse a layer instance several times. When we call a layer instance twice, instead of instantiating a new layer for each call, you reuse the same weights with every call. This allows you to build models that have shared branches—several branches that all share the same knowledge and perform the same operations. That is, they share the same representations and learn these representations simultaneously for different sets of inputs.\n",
    "\n",
    "For example, consider a model that attempts to assess the semantic similarity between two sentences. The model has two inputs (the two sentences to compare) and outputs a score between 0 and 1, where 0 means unrelated sentences and 1 means sentences that are either identical or reformulations of each other. Such a model could be useful in many applications, including deduplicating natural-language queries in a dialog system.\n",
    "\n",
    "In this setup, the two input sentences are interchangeable, because semantic similarity is a symmetrical relationship: the similarity of A to B is identical to the similarity of B to A. For this reason, it wouldn’t make sense to learn two independent models for processing each input sentence. Rather, you want to process both with a single LSTM layer. The representations of this LSTM layer (its weights) are learned based on both inputs simultaneously. This is what we call a Siamese LSTM model or a shared LSTM.\n",
    "\n",
    "Here’s how to implement such a model using layer sharing (layer reuse) in the Keras functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "\n",
    "lstm = layers.LSTM(32)                                                \n",
    "left_input = Input(shape=(None, 128))                                 \n",
    "left_output = lstm(left_input)                                        \n",
    "\n",
    "right_input = Input(shape=(None, 128))                                \n",
    "right_output = lstm(right_input)                                      \n",
    "\n",
    "merged = layers.concatenate([left_output, right_output], axis=-1)     \n",
    "predictions = layers.Dense(1, activation='sigmoid')(merged)           \n",
    "\n",
    "model = Model([left_input, right_input], predictions)                 \n",
    "model.fit([left_data, right_data], targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, a layer instance may be used more than once - it can be called arbitrarily many times, reusing the same set of weights every time.\n",
    "\n",
    "## 7.1.6 Models as layers\n",
    "Importantly, in the functional API, models can be used as you’d use layers—effectively, you can think of a model as a “bigger layer.” This is true of both the Sequential and Model classes. This means we can call a model on an input tensor and retrieve an output tensor:\n",
    "\n",
    "`y = model(x)`\n",
    "\n",
    "If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors:\n",
    "\n",
    "`y1, y2 = model([x1, x2])`\n",
    "\n",
    "When we call a model instance, we’re reusing the weights of the model—exactly like what happens when we call a layer instance. Calling an instance, whether it’s a layer instance or a model instance, will always reuse the existing learned representations of the instance—which is intuitive.\n",
    "\n",
    "One simple practical example of what we can build by reusing a model instance is a vision model that uses a dual camera as its input: two parallel cameras, a few centimeters (one inch) apart. Such a model can perceive depth, which can be useful in many applications. We shouldn’t need two independent models to extract visual features from the left camera and the right camera before merging the two feeds. Such low-level processing can be shared across the two inputs: that is, done via layers that use the same weights and thus share the same representations. Here’s how to implement a Siamese vision model (shared convolutional base) in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "xception_base = applications.Xception(weights=None, include_top=False)      \n",
    "\n",
    "left_input = Input(shape=(250, 250, 3))                       \n",
    "right_input = Input(shape=(250, 250, 3))                      \n",
    "\n",
    "left_features = xception_base(left_input)                     \n",
    "right_input = xception_base(right_input)                      \n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.7 Wrapping up\n",
    "This concludes our introduction to the Keras functional API—an essential tool for building advanced deep neural network architectures. Now we know the following:\n",
    "\n",
    " - To step out of the Sequential API whenever we need anything more than a linear stack of layers\n",
    " - How to build Keras models with several inputs, several outputs, and complex internal network topology, using the Keras functional API\n",
    " - How to reuse the weights of a layer or model across different processing branches, by calling the same layer or model instance several times"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
