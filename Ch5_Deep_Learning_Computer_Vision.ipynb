{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for computer vision\n",
    "This section covers convolutional neural networks (*convnets*), which is a type of deep learning model that is almost universally used in computer vision applications.\n",
    "\n",
    "## 5.1 Introduction to convnets\n",
    "Let's start by taking a practical look at a simple convnet example used to classify MNIST digits. In a previous model, we used a densely connected network that achieved a test accuracy of 97.8%. Let's see how this convnet model does on the same task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For its input a convnet takes input tensors of shape (`image_height, image_width, image_channels`). In this case, we have images that are 28x28 pixels, and only a black/white channel.\n",
    "\n",
    "Let's view the architecture of the convnet so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the width and height dimensions tend to shrink as you go deeper into the network. \n",
    "\n",
    "The next step is to feed the last output tensor (shape (3, 3, 64)) into a densely connected classifier network: a stack of `Dense` layers. These classifiers process 1D vectors, whereas the current output is a 3D tensor. To convert the 3D tensors into 1D vectors, we need to flatten them, and then add a few `Dense` layers on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do 10-way classification by using a final layers with 10 outputs and a softmax activation. Here is what the network looks like so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the (3, 3, 64) output tensors are flattened into vectors of shape (576,) before doing into two `Dense` layers.\n",
    "\n",
    "Now, let's train the convnet on the MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change the format of the training & testing images\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Change dtype and convert to a 0-1 scale\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# Set labels to categorical one-hot-encoded\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 79s - loss: 0.1729 - acc: 0.9456    \n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 81s - loss: 0.0477 - acc: 0.9847    \n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 80s - loss: 0.0322 - acc: 0.9901    \n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 88s - loss: 0.0251 - acc: 0.9925    - E\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 84s - loss: 0.0189 - acc: 0.9942    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10f83f650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit the model to training data\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99150000000000005"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model on test data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The densely connected network from a previous example had a test accuracy of 97.8%, but the basic convnet has a test accuracy of **99.2%**! Great improvement!\n",
    "\n",
    "To get deeper into the topic, we must ask why this simple convnet works so well, compared to a densely connected model? Let's dive into what the `Conv2D` and `MaxPooling2D` layers actually do.\n",
    "\n",
    "### 5.1.1 The convolution operation\n",
    "The core difference between a densely connected layer and a convolutional layer is this: `Dense` layers learn *global* patterns in their input feature space, whereas convolutional layers learn *local* patterns. In this case of images, those patterns are found in small 2D windows of the inputs shown below:\n",
    "\n",
    "![cnn_digit](images/5_1_1_cnn_digit.jpg)\n",
    "\n",
    "This key characteristic gives convnets two interesting properties:\n",
    " - *The patterns they learn are translation invariant*. After learning a pattern in any part of the picture, a convnet can recognize it anywhere. A densely connected network would have to learn the pattern over again if it appeared in a new location. This enables convnets to be trained on fewer samples to learn representations that have generalization power.\n",
    " - *They can learn spatial hierarchies of patterns*. A first convolutional layer will learn small local patterns such as edges, a secons convolutional layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts.\n",
    " \n",
    "![cat](images/5_1_1_cat.jpg)\n",
    "\n",
    "Convolutions operate over 3D tensors, called **feature maps**, with height, width, and depth (channels) axes. For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. The convolution extracts patches from its input feature map and applies the same transformation to all of these patches, producing an **output feature map**. The different channels of the output depth axis no longer stand for specific colors is in RGB, but they stand for *filters*. Filters encode specific aspects of the input data, such as presence of a face.\n",
    "\n",
    "In the MNIST example, the first convolutional layer takes a (28, 28, 1) sized feature map and outputs a feature map of size (26, 26, 32). It computes 32 filters over its input. Each of the 32 output channels contains a 26x26 grid of values, which is a *response map* of the filter over the input, indicating the response of that filter pattern at different locations in the input. Feature map means: every dimension in the depth axis is a feature (or filter), and the 2D tensor output is the 2D spatial map of the response of this filter over the input. The image below shows a single filter that detects for diagonal edges.\n",
    "\n",
    "![response_map](images/5_1_1_responsemap.jpg)\n",
    "\n",
    "Convolutions are defined by two key parameters:\n",
    " - **Size of the patches extracted from the inputs**- Typically 3x3 or 5x5.\n",
    " - **Depth of the output feature map**- Number of filters computed by the convolution.\n",
    " \n",
    "In Keras `Conv2D` layers, these parameters are the first arguments passed to the layer:\n",
    "\n",
    "`Conv2D(output_depth, (window_height, window_width))`\n",
    "\n",
    "A convolution works by sliding these 3x3 (or 5x5) windows over the 3D input feature map, stopping at every possible location and extracting the 3D patch of surrounding features (`window_height, window_width, input_depth`). Each 3D patch is transformed into a 1D vector (`output_depth,`). All these vectors are then spatially reassembled into a 3D output map of shape (`height, width, output_depth`). Every spatial location in the output feature map corresponds to the same location in the input feature map. \n",
    "\n",
    "![convolution](images/5_1_1_convolution.jpg)\n",
    "\n",
    "The output width and height may differ from the input width and height because can be attributed to border effects, or the use of *strides*.\n",
    "\n",
    "Let's take a deeper look at this once more:\n",
    "\n",
    "**UNDERSTANDING BORDER EFFECTS AND PADDING**\n",
    "Consider a 5x5 feature map. There are only 9 tiles which you can center a 3x3 window, forming a 3x3 grid. Hence, the output feature map will be 3x3. \n",
    "\n",
    "![patch](images/5_1_1_patch.jpg)\n",
    "\n",
    "If you want an output feature map with the same spatial dimensions as the input, you can use **padding**. Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so you can fit center convolution windows around every input tile.\n",
    "\n",
    "![padding](images/5_1_1_padding.jpg)\n",
    "\n",
    "**UNDERSTANDING CONVOLUTION STRIDES**\n",
    "The other factor that can influence output size is the notion of **strides**. So far, we have described convolutions assuming the center tiles of the convolution windows are all contiguous. The distance between two successive windows is a parameter of the convolution, called the stride, with a default value of 1. It's possible to have *strided convolutions* with a stride greater than 1. The image below shows a convolution with a stride of 2.\n",
    "\n",
    "![stride](images/5_1_1_stride.jpg)\n",
    "\n",
    "Using stride 2 means the width and height of the feature map are downsampled by a factor of 2. Strided convolutions are rarely used in practice, however. To downsample feature maps, we instead tend to use **max-pooling**.\n",
    "\n",
    "### 5.1.2 The max-pooling operation\n",
    "The role of max pooling is to aggressively downsample feature maps, much like strided convolutions. Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. This is similar to convolution, but instead of transforming local patches via a learned linear transformation, they are transformed via a hardcoded max tensor operation. Max pooling is usually done with 2x2 windows and stride 2, in order to downsample the feature maps by a factor of 2. Conversely, convolution is typically done with a 3x3 window and a stride of 1.\n",
    "\n",
    "Let's view this is coded context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_no_max_pool = models.Sequential()\n",
    "model_no_max_pool.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model_no_max_pool.add(layers.Conv2D(64, (3,3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 22, 22, 64)        36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_no_max_pool.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's wrong with this setup? Two things:\n",
    " - It doesn't allow a learning of spatial hierarchy of features. The 3x3 windows in the third layer will only contain information coming from 7x7 windows in the initial input. The high-level patterns learned by the convnet will still be very small with regard to the initial input, which may not be enough to learn to classify digits. We need features from the last convolution layer to contain information about the totality of the input.\n",
    " - The final feature map has 22x22x64 = 30,976 total coefficients per sample. This is huge. If you were to flatten it to stick a `Dense` layer of size 512 on top, that layer would have 15.8 million parameters. This is far too large for such a small model and would result in intense overfitting.\n",
    " \n",
    "The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows.\n",
    "\n",
    "Max pooling tends to work better than most alternatives because features tend to encode the spatial presence of some pattern or concept over the different tiles of the feature map, and it's more informative to look at the maximal presence of different features than at their average presence. The most reasonable subsampling strategy is to first produce dense maps of features and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs (strided convolutions), which could cause you to miss or dilute feature-presence information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
