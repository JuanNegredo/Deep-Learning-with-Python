{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 8.2 - *DeepDream*\n",
    "\n",
    "DeepDream is an artistic image-modification technique that uses the representations learned by convolutional neural networks. It quickly became a sensation thanks to the trippy pictures it could generate filled with bird feathers and dog eyes. This can be attributed to the fact that DeepDream was trained on ImageNet, where dogs and birds are aplenty (see below).\n",
    "\n",
    "![deepdream](images/8_2_0_DeepDream.png)\n",
    "\n",
    "The DeepDream algorithm is almost identical to the convnet filter-visualization technique, consisting of running a convnet in reverse: doing gradient ascent on the input to the convnet in order to maximize the activation of a specific filter in an upper layer of the convnet. DeepDream uses this same idea, with a few simple differences:\n",
    "\n",
    " - With DeepDream, you try to maximize the activation of entire layers rather than that of a specific filter, thus mixing together visualizations of large numbers of features at once.\n",
    " - You start not from blank, slightly noisy input, but rather from an existing image—thus the resulting effects latch on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.\n",
    " - The input images are processed at different scales (called octaves), which improves the quality of the visualizations.\n",
    " \n",
    "Now let's make some DeepDreams!\n",
    "\n",
    "## 8.2.1 Implementing DeepDream in Keras\n",
    "We’ll start from a convnet pretrained on ImageNet. Many such convnets are available in Keras: VGG16, VGG19, Xception, ResNet50, and so on. We can implement DeepDream with any of them, but our convnet of choice will naturally affect our visualizations. The convnet used in the original DeepDream release was an Inception model, so we’ll use the Inception V3 model that comes with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87891968/87910968 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.applications import inception_v3\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "model = inception_v3.InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the loss, which is the quantity we will seek to maximize during the gradient-ascent process. To do this, we'll simultaneously maximize the activation of all filters in a number of layers. Specifically, we will maximize a weighted sum of the L2 norm of the activations of a set of high-level layers. Lower layers result in geometric patterns, whereas higher layers result in visuals in which you can recognize some classes from ImageNet. We will start from an arbitrary configuration involving four layers.\n",
    "\n",
    "**SETTING UP THE DEEPDREAM CONFIGURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dictionary mapping layer names to a coefficient quantifying how much the layer's\n",
    "# activation contributes to the loss we want to maximize.\n",
    "\n",
    "layer_contributions = {\n",
    "    'mixed2': 0.2,\n",
    "    'mixed3': 3.,\n",
    "    'mixed4': 2.,\n",
    "    'mixed5': 1.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now define a tensor that contains the loss: the weighted sum of the L2 norm of the activations of the layers\n",
    "\n",
    "# create dict that maps layer names to layer instances\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "\n",
    "# define the loss by adding layer contributions to this scalar variable\n",
    "loss = K.variable(0.)\n",
    "for layer_name in layer_contributions:\n",
    "    coeff = layer_contributions[layer_name]\n",
    "    activation = layer_dict[layer_name].output # revive the layers output\n",
    "    \n",
    "    scaling = K.prod(K.cast(K.shape(activation), 'float32'))\n",
    "    loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling\n",
    "    # ^^ adds the L2 norm of features of a layer to the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRADIENT-ASCENT PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This tensor holds the generated image\n",
    "dream = model.input\n",
    "\n",
    "# Computes the gradients of the dream with regard to loss\n",
    "grads = K.gradients(loss, dream)[0]\n",
    "\n",
    "# Normalizes the gradients (important trick!)\n",
    "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7)\n",
    "\n",
    "# Set up a Keras function to retrieve the value of loss & gradients\n",
    "outputs = [loss, grads]\n",
    "fetch_loss_and_grads = K.function([dream], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    outs = fetch_loss_and_grads([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1]\n",
    "    return loss_value, grad_values\n",
    "\n",
    "# This function runs gradient ascent for a number of iterations\n",
    "def gradient_ascent(x, iterations, step, max_loss=None):\n",
    "    for i in range(iterations):\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        if max_loss is not None and loss_value > max_loss:\n",
    "            break\n",
    "        print('...Loss value at', i, ':', loss_value)\n",
    "        x += step * grad_values\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the actual DeepDream algorithm. First, we will define a list of scales (or octaves) at which to process the images. Each successive scale is larger than the previous one by a factor of 1.4. We start by processing a small image and then increasingly scale it up (see image below).\n",
    "\n",
    "![deepdreamprocess](images/8_2_1_process.jpg)\n",
    "\n",
    "For each successive scale, from smallest to the largest, we will run gradient ascent to maximize the loss that was previously defined. After each gradient ascent run, we upscale the resulting image by 40%.\n",
    "\n",
    "To avoid losing a lot of image detail after each successive scale-up, we can use a simple trick: after each scale-up, we’ll reinject the lost details back into the image, which is possible because we know what the original image should look like. Given a small image size S and a larger image size L, we can compute the difference between the original image resized to size L and the original resized to size S. This difference quantifies the details lost when going from S to L.\n",
    "\n",
    "**RUNNING THE GRADIENT ASCENT OVER DIFFERENT SUCCESSIVE SCALES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with these hyperparameters will allow us to achieve new effects\n",
    "step = 0.01 # gradient ascent step size\n",
    "num_octave = 3 # number of scales to run gradient ascent\n",
    "octave_scale = 1.4 # size ratio between scales\n",
    "iterations = 20 # number of ascent steps to run at each scale\n",
    "\n",
    "# if loss grows larger than 10, we interupt to avoid ugly artifacts\n",
    "max_loss = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "import scipy\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def resize_img(img, size):\n",
    "    img = np.copy(img)\n",
    "    factors = (1,\n",
    "               float(size[0]) / img.shape[1],\n",
    "               float(size[1]) / img.shape[2],\n",
    "               1)\n",
    "    return scipy.ndimage.zoom(img, factors, order=1)\n",
    "\n",
    "def save_img(img, fname):\n",
    "    pil_img = deprocess_image(np.copy(img))\n",
    "    scipy.misc.imsave(fname, pil_img)\n",
    "\n",
    "# Util function to open, resize, and format pictures into tensors that Inception can process\n",
    "def preprocess_image(image_path):\n",
    "    img = image.load_img(image_path)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Util function to convert tensor into a valid image\n",
    "def deprocess_image(x):\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.reshape((3, x.shape[2], x.shape[3]))\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[1], x.shape[2], 3)) # undoes propocessing performed by inception_v3.proprocess_input\n",
    "    x /= 2.\n",
    "    x += 0.5\n",
    "    x *= 255.\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of image we want to use\n",
    "base_image_path = 'images/rlatimer.jpg'\n",
    "\n",
    "# Load the base image into a Numpy array\n",
    "img = preprocess_image(base_image_path)\n",
    "\n",
    "# Prepare a list of shape tuples defining the diff scales to run grad asc.\n",
    "original_shape = img.shape[1:3]\n",
    "successive_shapes = [original_shape]\n",
    "for i in range(1, num_octave):\n",
    "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
    "    successive_shapes.append(shape)\n",
    "    \n",
    "# Reverse the list of shapes so they're in increasing order\n",
    "successive_shapes = successive_shapes[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing image shape', (1175, 1175))\n",
      "('...Loss value at', 0, ':', 1.5334526)\n",
      "('...Loss value at', 1, ':', 2.1093473)\n",
      "('...Loss value at', 2, ':', 2.860016)\n",
      "('...Loss value at', 3, ':', 3.751883)\n",
      "('...Loss value at', 4, ':', 4.6573687)\n",
      "('...Loss value at', 5, ':', 5.56007)\n",
      "('...Loss value at', 6, ':', 6.4297724)\n",
      "('...Loss value at', 7, ':', 7.250205)\n",
      "('...Loss value at', 8, ':', 8.061167)\n",
      "('...Loss value at', 9, ':', 8.838395)\n",
      "('...Loss value at', 10, ':', 9.5841465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/RobStorage/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Processing image shape', (1645, 1645))\n",
      "('...Loss value at', 0, ':', 3.6731281)\n",
      "('...Loss value at', 1, ':', 5.0932336)\n",
      "('...Loss value at', 2, ':', 6.2126055)\n",
      "('...Loss value at', 3, ':', 7.2114797)\n",
      "('...Loss value at', 4, ':', 8.140279)\n",
      "('...Loss value at', 5, ':', 9.023996)\n",
      "('...Loss value at', 6, ':', 9.836564)\n",
      "('Processing image shape', (2304, 2304))\n",
      "('...Loss value at', 0, ':', 3.704988)\n",
      "('...Loss value at', 1, ':', 5.058856)\n",
      "('...Loss value at', 2, ':', 6.1736317)\n",
      "('...Loss value at', 3, ':', 7.187393)\n",
      "('...Loss value at', 4, ':', 8.1419)\n",
      "('...Loss value at', 5, ':', 9.051799)\n",
      "('...Loss value at', 6, ':', 9.908897)\n"
     ]
    }
   ],
   "source": [
    "# Resize Numpy array of the image to the smallest scale\n",
    "original_img = np.copy(img)\n",
    "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
    "\n",
    "for shape in successive_shapes:\n",
    "    print('Processing image shape', shape)\n",
    "    img = resize_img(img, shape) # scales up the dream image\n",
    "    \n",
    "    # Run gradient ascent, altering the dream image\n",
    "    img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss)\n",
    "    \n",
    "    # Scale up the smaller version of original image\n",
    "    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
    "    \n",
    "    # Compute the high-quality version of the original\n",
    "    same_size_original = resize_img(original_img, shape)\n",
    "    \n",
    "    # Difference between the two is the detail that was lost when scaling up\n",
    "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
    "    \n",
    "    # Reinjects list detail into Dream\n",
    "    img += lost_detail\n",
    "    shrunk_original_img = resize_img(original_img, shape)\n",
    "    save_img(img, fname='dream_at_scale_' + str(shape) + '.png')\n",
    "    \n",
    "save_img(img, fname='final_dream.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Because the original Inception V3 network was trained to recognize concepts in images of size 299 × 299, and given that the process involves scaling the images down by a reasonable factor, the DeepDream implementation produces much better results on images that are somewhere between 300 × 300 and 400 × 400. Regardless, we can run the same code on images of any size and any ratio.\n",
    "\n",
    "I started with this picture of me:\n",
    "\n",
    "![starting_image](images/rlatimer.jpg)\n",
    "\n",
    "And here is our DeepDream output!\n",
    "\n",
    "![output_image](images/final_dream.png)\n",
    "\n",
    "Now that we have an introduction into DeepDream, it would be valuable to explore what we can do by adjusting which layers we use in our loss. Layers that are lower in the network contain more-local, less-abstract representations and lead to dream patterns that look more geometric. Layers that are higher up lead to more-recognizable visual patterns based on the most common objects found in ImageNet. We can use random generation of the parameters in the `layer_contributions` dictionary to quickly explore many different layer combinations. The image below shows a range of results obtained using different layer configurations, from an image of a batch of pastries.\n",
    "\n",
    "![pastries](images/8_2_1_pastries.png)\n",
    "\n",
    "## 8.2.2 Wrapping up\n",
    " - DeepDream consists of running a convnet in reverse to generate inputs based on the representations learned by the network.\n",
    " - The results produced are fun and somewhat similar to the visual artifacts induced in humans by the disruption of the visual cortex via psychedelics.\n",
    " - Note that the process isn’t specific to image models or even to convnets. It can be done for speech, music, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
